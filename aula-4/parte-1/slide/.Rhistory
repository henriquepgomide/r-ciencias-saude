?hist
install.packages("lavaan")
library("lavaan", lib.loc="/home/hpgomide/R/x86_64-pc-linux-gnu-library/3.0")
detach("package:lavaan", unload=TRUE)
crrjf_final_1.0 <- read.csv("~/Dropbox/CRRJF/bancos_crrjf/crrjf_final_1.0.csv", sep=";")
View(crrjf_final_1.0)
years  <- ("2002", "2004", "2006", "2009", "2009", "2009", "2011", "2011","2011","2012", "2012")
years  <- ("2002", "2004", "2006", "2009", "2009", "2009", "2011", "2011","2011","2012", "2012")
years  <- c("2002", "2004", "2006", "2009", "2009", "2009", "2011", "2011","2011","2012", "2012")
table(years)
?compare
a  <- c(1,2,3,4)
b  <- c(1,2,3,3)
data  <- data.frame(row.names = c("varA", "varB"), a,b)
data  <- data.frame(a,b)
str(data)
is.element(1, data$a)
is.element(1, data$a[1])
is.element(1, data$a[2])
is.element(data$a[1], data$b[1])
is.element(data$a[4], data$b[4])
for (i in 4) {
is.element(data$a[i], data$b[i])
}
for (i in 4) {
out[i]  <- is.element(data$a[i], data$b[i])
print  <- out[i]
}
out  <- is.element(data$a[i], data$b[i])
for (i in 4) {
out  <- is.element(data$a[i], data$b[i])
print  <- out[i]
}
for (i in 4) {
is.element(data$a[i], data$b[i])
print[i]
}
for (i in 4) {
is.element(data$a[i], data$b[i])
print[i]
}
for (i in 4) {
c$[i]  <- is.element(data$a[i], data$b[i])
c
}
for (i in 4) {
output$[i]  <- is.element(data$a[i], data$b[i])
output
}
for (i in 1:4) {
is.element(data$a[i], data$b[i])
}
for (i in 1:4) {
out[i]  <- is.element(data$a[i], data$b[i])
print(out)
}
out
cbind(out)
for (i in 1:4) {
out  <- is.element(data$a[i], data$b[i])
print(out)
}
for (i in 1:4) {
out  <- is.element(data$a[i], data$b[i])
print(out)
}
lenght(a)
length  <- data$a
length(data$a)
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
print(out)
}
a  <- c(1,2,3,4,5)
b  <- c(1,2,3,3,4)
data  <- data.frame(a,b)
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
print(out)
}
?lapply
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
if (out  = TRUE) {
out  <- "Correto"
}
out  <- "Conferir"
}
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
if (out  == TRUE) {
out  <- "Correto"
}
else  out  <- "Conferir"
}
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
if (out  == TRUE) {
out  <- "Correto"
}
else  out  <- "Conferir"
out
}
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
out
}
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
print(out)
}
a  <- c(1,2,3,4,5)
b  <- c(1,2,3,3,4)
data  <- data.frame(a,b)
for (i in 1:length(data$a) ) {
out$[i]  <- is.element(data$a[i], data$b[i])
print(out)
}
for (i in 1:length(data$a) ) {
out[i]  <- is.element(data$a[i], data$b[i])
print(out)
}
for (i in 1:length(data$a) ) {
data.frame(out)
out  <- is.element(data$a[i], data$b[i])
print(out)
}
for (i in 1:length(data$a) ) {
out  <- is.element(data$a[i], data$b[i])
print(out)
}
data.a  <- data.frame(a,b)
data.b  <- data.frame(b,a)
length(data.a)
str(data.a)
nobs(data.a)
nrows(data.a)
rowsum(data.a)
row(data.a)
levels(data.a)
nrows(data.a)
nrow(data.a)
data.a  <- data.frame(a,b)
data.b  <- data.frame(a,b)
require(RCurl)
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Au9zi20yTsCSdGZFRzhyREVOOEhLSkM2YVAxRW5kUHc&single=true&gid=0&output=csv")
dados  <- read.csv(textConnection(meuCsV))
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&output=csv")
dados  <- read.csv(textConnection(meuCsV))
dados  <- read.csv(textConnection(meuCsv))
names(dados)
dados  <- read.csv(textConnection(meuCsv), header=T, fill="NA")
dados  <- read.csv(textConnection(meuCsv), header=T, fill=TRUE)
dados[1:2,0]
dados[1,1:2]
dados  <- read.table(textConnection(meuCsv), header=T)
data  <- read.table("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&output=csv", sep=",", header = TRUE)
dados  <- read.csv(textConnection(meuCsv), header=T)
?read.csv
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE)
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("V", seq_len(ncol)))
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste("V", seq_len(ncol)))
dados  <- read.csv(textConnection(meuCsv), header=T, col.names = paste0("V", seq_len(ncol)))
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE)
ncol  <- max(count.fields(dados))
ncol  <- max(count.fields(textConnection(meuCsv)))
ncol
ncol  <- max(count.fields(textConnection(meuCsv), sep =","))
ncol
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("V", seq_len(ncol)))
names(dados[,1:5])
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)))
?paste
a  <- paste(1,2)
a
a  <- paste0(1,2)
a
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)))
names(dados[,1:5])
levels(dados)
levels(dados$1)
levels(dados$v5)
levels(dados$v6)
head(dados[,1:5])
head(dados[,1:10])
View(dados)
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA" )
head(dados[,1:10])
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = " " )
head(dados[,1:10])
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA)
head(dados[,1:10])
summary(dados$v2)
str(dados$v2)
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA)
dados$v2
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&output=csv")
ncol  <- max(count.fields(textConnection(meuCsv), sep =","))
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA)
dados$v2
dados$v3
dados$v4
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, enconding = "UTF-8")
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, enconding="UTF-8")
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA)
dados$v4
dados$v5
dados$v6
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, encoding="latin1")
dados$v6
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, encoding="UTF-8")
dados$v6
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&output=csv")
ncol  <- max(count.fields(textConnection(meuCsv), sep =","))
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, encoding="UTF-8")
dados$v6
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&single=true&gid=1&output=csv")
ncol  <- max(count.fields(textConnection(meuCsv), sep =","))
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, encoding="UTF-8")
dados$v6
table(dados$v6)
levels(dados$v6)
str(dados$v6)
sapply(dados, srt)
sapply(dados, str)
sapply(dados, levels)
sapply(dados, levels, simplify=TRUE)
table(dados$v6)
dados  <- read.csv(textConnection(meuCsv), header=T, fill = FALSE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, encoding="UTF-8")
table(dados$v6)
head(dados[,1:6])
head(dados[,1:10])
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = "NA", colClasses = NA, encoding="UTF-8")
head(dados[,1:10])
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = c("NA",""), colClasses = NA)
head(dados[,1:10])
dados  <- read.csv(textConnection(meuCsv), header=T, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = c("NA",""))
head(dados[,1:10])
summary(dados$v2)
table(dados$v4)
cbind(table(dados$v4))
install.packages("pwr")
library(pwer)
library(pwr)
?pwer
?pwr
?pwr.chisq.test
pwr.chisq.test(N= 16, df=3, sig.level=.05, power=.8)
cohen.ES(test="chisq", size=small)
cohen.ES(test="chisq", size="small")
cohen.ES(test="chisq", size="large")
pwr.chisq.test(N= 16, df=1, sig.level=.05, power=.8)
pwr.chisq.test(N= 16, df=1, sig.level=.05, power=.7)
cohen.ES(test="chisq", size="medium")
cohen.ES(test="chisq", size="large")
cite("pwr")
cite()
citation()
citation(pwr)
citation("pwr")
library(RCurl)# Carregando pacote RCurl
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&single=true&gid=1&output=csv")  #salvando o link para a planilha do Google
ncol  <- max(count.fields(textConnection(meuCsv), sep =",")) # Estimando o número de colunas da tabela
install.packages("RCurl") # Instalando o pacote RCurl
library(RCurl)# Carregando pacote RCurl
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&single=true&gid=1&output=csv")  #salvando o link para a planilha do Google
ncol  <- max(count.fields(textConnection(meuCsv), sep =",")) # Estimando o número de colunas da tabela
dados  <- read.csv(textConnection(meuCsv), header=TRUE, fill = TRUE, col.names = paste0("v", seq_len(ncol)), na.strings = c("NA",""))
install.packages("car")
library("car")
head(dados)
names(dados)
dados[1:3,] # casos de um a três
dados[,1:3] # ver três primeiras variáveis de todos os casos
dados[1:10, 1:3] # ver trẽs primeiras variáveis dos 10 primeiros casos
dados[1:10, c("v1", "v40", "v42")] # ver variáveis v1, v40, v42 dos 10 primeiros casos
str(dados$v2)
summary(dados$v3)
summary(dados$v2)
levels(dados$v2)
levels(dados$v3)
summary(dados$ex1)
dados$ex1[dados$v2<=20] <- "Menor ou igual a Vinte"
dados$ex1[dados$v2>20] <- "Maior que Vinte"
summary(dados$ex1)
table(dados$ex1)
table(dados$v3)
levels(dados$v3)
dados$ex2[dados$v3 == "Feminino"]  <- "Mulher"
dados$ex2[dados$v3 == "Masculino"]  <- "Homem"
table(dados$ex2)
dados$ex3[dados$v3 == "Feminino" & dados$v2 < 21]  <- "Mulher com menos de 21 anos"
dados$ex3[dados$v3 == "Feminino" & dados$v2 >= 21]  <- "Mulher com 21 ou mais anos"
table(dados$ex3)
library(RCurl)
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0Atv-ntf9TI4edFpMalBzelZxa1dxZFByR29FVUFjM3c&single=true&gid=1&output=csv", ssl.verifypeer = FALSE)  #salvando o link para a planilha do Google
library("RCurl")
?RCurl
??RCurl
meuCsv  <- getURL("https://www.informalcool.org.br/admin/reports/export_audit", ssl.verifypeer = FALSE)
meuCsv  <- getURL("https://www.informalcool.org.br/admin/reports/export_audit", ssl.verifypeer = FALSE)
meuCsv  <- getURL("https://www.informalcool.org.br/admin/reports/export_audit")
read.csv("ftp103.redehost.com.br", header=TRUE)
library(RCurl)# Carregando pacote RCurl
meuCsv  <- getURL("https://docs.google.com/spreadsheet/pub?key=0AoPQFK9g2JridFlIYXZabWNRWXN0Um9mLVFOVzNKbXc&single=true&gid=0&output=csv", ssl.verifypeer = FALSE)  #salvando o link para a planilha do Google
ncol  <- max(count.fields(textConnection(meuCsv), sep =",")) # Estimando o número de colunas da tabela
dados  <- read.csv(file.choose(),  header=TRUE, sep=",", col.names = paste0("v", seq_len(ncol)), na.strings = c("NA",""), strip.white = TRUE)
summary(dados$v3) # Estatística para variável númerica, idade
quantile(dados$v3)
quantile(dados$v3, na.rm=TRUE)
?quantile
quantile(dados$v3,probs=c(.5,.75) , na.rm=TRUE)
quantile(dados$v3,probs=c(.1,.2,.5,.75) , na.rm=TRUE)
library(psych)
describe(dados$v3)
summary(dados$v3)
mytable <- table(dados$v6)
table(dados$v6)
prop.table(dados$v6)
table(dados$v6)
nrows(dados$v)
nrows(dados$v6)
nrow(dados$v6)
sum(dados$v6)
nrow(dados$v6)
nrow(dados)
sum(table(dados$v6))
table(dados$v6/sum(table(dados$v6)))
dados$v6/sum(table(dados$v6)
)
table(dados$v6)
table(dados$v6)/sum(table(dados$v6))
round(table(dados$v6)/sum(table(dados$v6)),2)
round(cbind(table(dados$v6)/sum(table(dados$v6))),2)
f.table  <- function(var) {
round(cbind(table(var)/sum(table(dados$v6))), 2)
}
f.table(dados$v6)
f.table  <- function(var) {
round(cbind(sort(table(var)/sum(table(dados$v6)), decreasing=TRUE)), 2)
}
f.table(dados$v6)
round(cbind(sort(table(var)/sum(table(dados$v6)*100), decreasing=TRUE)), 2)
f.table  <- function(var) {
round(cbind(sort(table(var)/sum(table(dados$v6)*100), decreasing=TRUE)), 2)
}
f.table(dados$v6)
f.table  <- function(var) {
round(cbind(sort(table(var)/sum(table(dados$v6)), decreasing=TRUE)), 2)
}
f.table(dados$v6)
describe
summary
alpha
estigma  <- read.csv("~/Dropbox/Mestrado/artigo-estigma/estigma.csv", header =TRUE, sep = ",")
cortest.bartlett(data.c.distsoc) #Escala ok
library("psych")
library("GPArotation")
library("pastecs")
library("corpcor")
library("boot")
kmo = function( data ){
kmo = function( data ){
library(MASS)
X <- cor(as.matrix(data))
iX <- ginv(X)
S2 <- diag(diag((iX^-1)))
AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
IS <- X+AIS-2*S2                         # image covariance matrix
Dai <- sqrt(diag(diag(AIS)))
IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
AA <- sum(a)
b <- apply((X - diag(nrow(X)))^2, 2, sum)
BB <- sum(b)
MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the correlation matrix. That is the  negative of the partial correlations, partialling out all other variables.
kmo <- BB/(AA+BB)                     # overall KMO statistic
# Reporting the conclusion
if (kmo >= 0.00 && kmo < 0.50){test <- 'The KMO test yields a degree of common variance unacceptable for FA.'}
else if (kmo >= 0.50 && kmo < 0.60){test <- 'The KMO test yields a degree of common variance miserable.'}
else if (kmo >= 0.60 && kmo < 0.70){test <- 'The KMO test yields a degree of common variance mediocre.'}
else if (kmo >= 0.70 && kmo < 0.80){test <- 'The KMO test yields a degree of common variance middling.' }
else if (kmo >= 0.80 && kmo < 0.90){test <- 'The KMO test yields a degree of common variance meritorious.' }
else { test <- 'The KMO test yields a degree of common variance marvelous.' }
ans <- list( overall = kmo,
report = test,
individual = MSA,
AIS = AIS,
AIR = AIR )
return(ans)
}
kmo = function( data ){
library(MASS)
X <- cor(as.matrix(data))
iX <- ginv(X)
S2 <- diag(diag((iX^-1)))
AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
IS <- X+AIS-2*S2                         # image covariance matrix
Dai <- sqrt(diag(diag(AIS)))
IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
AA <- sum(a)
b <- apply((X - diag(nrow(X)))^2, 2, sum)
BB <- sum(b)
MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the correlation matrix. That is the  negative of the partial correlations, partialling out all other variables.
kmo <- BB/(AA+BB)                     # overall KMO statistic
# Reporting the conclusion
if (kmo >= 0.00 && kmo < 0.50){test <- 'The KMO test yields a degree of common variance unacceptable for FA.'}
else if (kmo >= 0.50 && kmo < 0.60){test <- 'The KMO test yields a degree of common variance miserable.'}
else if (kmo >= 0.60 && kmo < 0.70){test <- 'The KMO test yields a degree of common variance mediocre.'}
else if (kmo >= 0.70 && kmo < 0.80){test <- 'The KMO test yields a degree of common variance middling.' }
else if (kmo >= 0.80 && kmo < 0.90){test <- 'The KMO test yields a degree of common variance meritorious.' }
else { test <- 'The KMO test yields a degree of common variance marvelous.' }
ans <- list( overall = kmo,
report = test,
individual = MSA,
AIS = AIS,
AIR = AIR )
return(ans)
}
residuals.stats  <- function(matrix){
residuals  <- as.matrix(matrix[upper.tri(matrix)])
large.resid  <- abs(residuals) > 0.05
numberLargeResids  <- sum(large.resid)
propLargeResids  <- numberLargeResids/nrow(residuals)
rmsr <- sqrt(mean(residuals^2))
cat("Root Mean squared residual = ", rmsr, "\n")
cat("Number of absolute residuals > 0.05 = ", numberLargeResids, "\n")
cat("Proportion of absolute residuals > 0.05 = ", propLargeResids, "\n")
hist(residuals)
}
estigma$sum.c.distsoc  <-  estigma$c.distsoc1 + estigma$c.distsoc2 + estigma$c.distsoc3 + estigma$c.distsoc4 + estigma$c.distsoc5
estigma$sum.d.distsoc  <-  estigma$d.distsoc1 + estigma$d.distsoc2 + estigma$d.distsoc3 + estigma$d.distsoc4 + estigma$d.distsoc5
estigma$sum.e.distsoc  <-  estigma$e.distsoc1 + estigma$e.distsoc2 + estigma$e.distsoc3 + estigma$e.distsoc4 + estigma$e.distsoc5
c.distsoc  <- subset(estigma, sum.c.distsoc >= 1)
(*)
library("boot")
library("corpcor")
install.packages("corpcor")
library("corpcor", lib.loc="/home/hpgomide/R/x86_64-pc-linux-gnu-library/3.0")
library("psych")
library("GPArotation")
install.packages("GPArotation")
library("GPArotation")
library("pastecs")
install.packages("pastecs")
library("pastecs")
library("corpcor")
library("boot")
library("Matrix", lib.loc="/usr/lib/R/library")
kmo = function( data ){
library(MASS)
X <- cor(as.matrix(data))
iX <- ginv(X)
S2 <- diag(diag((iX^-1)))
AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
IS <- X+AIS-2*S2                         # image covariance matrix
Dai <- sqrt(diag(diag(AIS)))
IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
AA <- sum(a)
b <- apply((X - diag(nrow(X)))^2, 2, sum)
BB <- sum(b)
MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the correlation matrix. That is the  negative of the partial correlations, partialling out all other variables.
kmo <- BB/(AA+BB)                     # overall KMO statistic
# Reporting the conclusion
if (kmo >= 0.00 && kmo < 0.50){test <- 'The KMO test yields a degree of common variance unacceptable for FA.'}
else if (kmo >= 0.50 && kmo < 0.60){test <- 'The KMO test yields a degree of common variance miserable.'}
else if (kmo >= 0.60 && kmo < 0.70){test <- 'The KMO test yields a degree of common variance mediocre.'}
else if (kmo >= 0.70 && kmo < 0.80){test <- 'The KMO test yields a degree of common variance middling.' }
else if (kmo >= 0.80 && kmo < 0.90){test <- 'The KMO test yields a degree of common variance meritorious.' }
else { test <- 'The KMO test yields a degree of common variance marvelous.' }
ans <- list( overall = kmo,
report = test,
individual = MSA,
AIS = AIS,
AIR = AIR )
return(ans)
}
estigma  <- read.csv("~/Dropbox/Mestrado/artigo-estigma/estigma.csv", header =TRUE, sep = ",")
estigma$sum.c.distsoc  <-  estigma$c.distsoc1 + estigma$c.distsoc2 + estigma$c.distsoc3 + estigma$c.distsoc4 + estigma$c.distsoc5
estigma$sum.d.distsoc  <-  estigma$d.distsoc1 + estigma$d.distsoc2 + estigma$d.distsoc3 + estigma$d.distsoc4 + estigma$d.distsoc5
estigma$sum.e.distsoc  <-  estigma$e.distsoc1 + estigma$e.distsoc2 + estigma$e.distsoc3 + estigma$e.distsoc4 + estigma$e.distsoc5
c.distsoc  <- subset(estigma, sum.c.distsoc >= 1)
data.c.distsoc  <- data.frame(c.distsoc$c.distsoc1, c.distsoc$c.distsoc2, c.distsoc$c.distsoc3, c.distsoc$c.distsoc4, c.distsoc$c.distsoc5)
d.distsoc  <- subset(estigma, sum.d.distsoc >= 1)
data.d.distsoc  <- data.frame(d.distsoc$d.distsoc1, d.distsoc$d.distsoc2, d.distsoc$d.distsoc3, d.distsoc$d.distsoc4, d.distsoc$d.distsoc5)
e.distsoc  <- subset(estigma, sum.e.distsoc >= 1)
data.e.distsoc  <- data.frame(e.distsoc$e.distsoc1, e.distsoc$e.distsoc2, e.distsoc$e.distsoc3, e.distsoc$e.distsoc4, e.distsoc$e.distsoc5)
matrix.c  <- cor(data.c.distsoc)
matrix.d  <- cor(data.d.distsoc)
matrix.e  <- cor(data.e.distsoc)
cortest.bartlett(data.c.distsoc) #Escala ok
det(cor(data.c.distsoc)) #0.14
alpha(data.c.distsoc) #alfa 0.81
?alpha
alpha(data.d.distsoc)
alpha(data..distsoc)
alpha(data.e.distsoc)
setwd("~/Dropbox/cursos/r-psicologia/aula-4/parte-3/slide")
library("slidify")
publish_rpubs("Semana 4 - Mas mamãe, eu sou normal?")
setwd("~/Dropbox/cursos/r-psicologia/aula-4/parte-2/slide")
publish_rpubs("Semana 4 - Estatística descritiva - com figuras")
setwd("~/Dropbox/cursos/r-psicologia/aula-4/parte-1/slide")
publish_rpubs("Semana 4 - Estatística descritiva - sem figuras")
